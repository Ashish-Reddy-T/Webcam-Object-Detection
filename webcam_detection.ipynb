{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import colorsys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "\n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "        return self.label\n",
    "\n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "        return self.score\n",
    "    \n",
    "\n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2, x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2, x4) - x3\n",
    "\n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
    "\n",
    "    intersect = intersect_w * intersect_h\n",
    "\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "\n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "\n",
    "    return float(intersect) / union\n",
    "\n",
    "\n",
    "def preprocess_input(image_pil, net_h, net_w):\n",
    "    image = np.asarray(image_pil)\n",
    "    new_h, new_w, _ = image.shape\n",
    "\n",
    "    # (1) - starts  \n",
    "    if (float(net_w)/new_w) < (float(net_h)/new_h):\n",
    "        new_h = (new_h * net_w)/new_w\n",
    "        new_w = net_w\n",
    "    else:\n",
    "        new_w = (new_w * net_h)/new_h\n",
    "        new_h = net_h \n",
    "    # (1) - ends\n",
    "    \n",
    "    new_w = int(new_w)\n",
    "    new_h = int(new_h)\n",
    "\n",
    "    # (2) - starts\n",
    "    resized = cv2.resize(image/255., (int(new_w), int(new_h)))\n",
    "    # (2) - ends\n",
    "\n",
    "    # (3) - starts\n",
    "    new_image = np.ones((net_h, net_w, 3)) * 0.5\n",
    "    new_image[int((net_h-new_h)//2):int((net_h+new_h)//2), int((net_w-new_w)//2):int((net_w+new_w)//2), :] = resized\n",
    "    # (3) - ends\n",
    "\n",
    "    # (4) - starts\n",
    "    new_image = np.expand_dims(new_image, 0)\n",
    "    # (4) - ends\n",
    "\n",
    "    # The new image maintains aspect ratio - (1), normalizes pixel values (faster for neural networks) - (2), \n",
    "    # Adds padding (letterboxing) - (3), and expands batch dimension to make it compatible with deep learning models - (4) !!!\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def decode_netout(netout_, obj_thresh, anchors_, image_h, image_w, net_h, net_w):\n",
    "    netout_all = deepcopy(netout_)\n",
    "    boxes_all = []\n",
    "    for i in range(len(netout_all)):\n",
    "      netout = netout_all[i][0]\n",
    "      anchors = anchors_[i]\n",
    "\n",
    "      grid_h, grid_w = netout.shape[:2]\n",
    "      nb_box = 3\n",
    "      netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
    "      nb_class = netout.shape[-1] - 5\n",
    "\n",
    "      boxes = []\n",
    "\n",
    "      netout[..., :2]  = _sigmoid(netout[..., :2])\n",
    "      netout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
    "      netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
    "      netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
    "\n",
    "      for i in range(grid_h*grid_w):\n",
    "          row = i // grid_w\n",
    "          col = i % grid_w\n",
    "\n",
    "          for b in range(nb_box):\n",
    "              # 4th element is objectness score\n",
    "              objectness = netout[row][col][b][4]\n",
    "              #objectness = netout[..., :4]\n",
    "              # last elements are class probabilities\n",
    "              classes = netout[row][col][b][5:]\n",
    "\n",
    "              if((classes <= obj_thresh).all()): continue\n",
    "\n",
    "              # first 4 elements are x, y, w, and h\n",
    "              x, y, w, h = netout[row][col][b][:4]\n",
    "\n",
    "              x = (col + x) / grid_w # center position, unit: image width\n",
    "              y = (row + y) / grid_h # center position, unit: image height\n",
    "              w = anchors[b][0] * np.exp(w) / net_w # unit: image width\n",
    "              h = anchors[b][1] * np.exp(h) / net_h # unit: image height\n",
    "\n",
    "              box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
    "              #box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, None, classes)\n",
    "\n",
    "              boxes.append(box)\n",
    "\n",
    "      boxes_all += boxes\n",
    "\n",
    "    # Correct boxes\n",
    "    boxes_all = correct_yolo_boxes(boxes_all, image_h, image_w, net_h, net_w)\n",
    "\n",
    "    return boxes_all\n",
    "\n",
    "def correct_yolo_boxes(boxes_, image_h, image_w, net_h, net_w):\n",
    "    boxes = deepcopy(boxes_)\n",
    "    if (float(net_w)/image_w) < (float(net_h)/image_h):\n",
    "        new_w = net_w\n",
    "        new_h = (image_h*net_w)/image_w\n",
    "    else:\n",
    "        new_h = net_w\n",
    "        new_w = (image_w*net_h)/image_h\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
    "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
    "\n",
    "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
    "    return boxes\n",
    "\n",
    "def do_nms(boxes_, nms_thresh, obj_thresh):\n",
    "    boxes = deepcopy(boxes_)\n",
    "    if len(boxes) > 0:\n",
    "        num_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    for c in range(num_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "\n",
    "            if boxes[index_i].classes[c] == 0: continue # Skip if already suppressed\n",
    "\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0 # Suppress lower-confidence boxes\n",
    "\n",
    "    new_boxes = []\n",
    "    for box in boxes:\n",
    "        label = -1\n",
    "\n",
    "        for i in range(num_class):\n",
    "            if box.classes[i] > obj_thresh:\n",
    "                label = i\n",
    "                # print(\"{}: {}, ({}, {})\".format(labels[i], box.classes[i]*100, box.xmin, box.ymin))\n",
    "                box.label = label\n",
    "                box.score = box.classes[i]\n",
    "                new_boxes.append(box)\n",
    "\n",
    "    return new_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image_, boxes, labels):\n",
    "    image = image_.copy()\n",
    "    image_w, image_h = image.size\n",
    "    font = ImageFont.truetype(font='/Users/AshishR_T/Library/Fonts/Monogram Font.ttf',\n",
    "                    size=np.floor(3e-2 * image_h + 0.5).astype('int32'))\n",
    "    thickness = (image_w + image_h) // 300\n",
    "\n",
    "    # Generate colors for drawing bounding boxes.\n",
    "    hsv_tuples = [(x / len(labels), 1., 1.)\n",
    "                  for x in range(len(labels))]\n",
    "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "    colors = list(\n",
    "        map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
    "    np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
    "    np.random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n",
    "    np.random.seed(None)  # Reset seed to default.\n",
    "\n",
    "    for i, box in reversed(list(enumerate(boxes))):\n",
    "        c = box.get_label()\n",
    "        predicted_class = labels[c]\n",
    "        score = box.get_score()\n",
    "        top, left, bottom, right = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "\n",
    "        label = '{} {:.2f}'.format(predicted_class, score)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        label_size = draw.textbbox((0,0),label, font)\n",
    "        label_size = (label_size[2], label_size[3])\n",
    "\n",
    "        top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "        left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "        bottom = min(image_h, np.floor(bottom + 0.5).astype('int32'))\n",
    "        right = min(image_w, np.floor(right + 0.5).astype('int32'))\n",
    "        print(label, (left, top), (right, bottom))\n",
    "\n",
    "        if top - label_size[1] >= 0:\n",
    "            text_origin = np.array([left, top - label_size[1]])\n",
    "        else:\n",
    "            text_origin = np.array([left, top + 1])\n",
    "\n",
    "        if right > left and bottom > top:\n",
    "          # proceed to draw the box\n",
    "          draw.rectangle([left, top, right, bottom], outline=colors[c], width=thickness)\n",
    "        else:\n",
    "          print(f\"Invalid box: {(left, top, right, bottom)}\")\n",
    "\n",
    "        draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
    "        #draw.text(text_origin, label, fill=(0, 0, 0))\n",
    "        del draw\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AshishR_T/miniconda3/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "2025-02-23 19:16:56.027882: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-02-23 19:16:56.027924: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-02-23 19:16:56.027931: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2025-02-23 19:16:56.027953: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-23 19:16:56.027966: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#Yolo v3 model (pre-trained)\n",
    "model_path = 'YOLO_model.h5'\n",
    "darknet = tf.keras.models.load_model(model_path, compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \\\n",
    "              \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \\\n",
    "              \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \\\n",
    "              \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \\\n",
    "              \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \\\n",
    "              \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \\\n",
    "              \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \\\n",
    "              \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \\\n",
    "              \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \\\n",
    "              \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = [[[116,90], [156,198], [373,326]], [[30,61], [62,45], [59,119]], [[10,13], [16,30], [33,23]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_image(image_pil, obj_thresh = 0.4, nms_thresh = 0.45, darknet = darknet, net_h = 416, net_w = 416, anchors = anchors, labels = labels):\n",
    "  # Preprocessing\n",
    "  image_w, image_h = image_pil.size\n",
    "  new_image = preprocess_input(image_pil, net_h, net_w)\n",
    "\n",
    "  # DarkNet\n",
    "  yolo_outputs = darknet.predict(new_image)\n",
    "\n",
    "  # Decode the output of the network\n",
    "  boxes = decode_netout(yolo_outputs, obj_thresh, anchors, image_h, image_w, net_h, net_w)\n",
    "\n",
    "  # Suppress non-maximal boxes\n",
    "  boxes = do_nms(boxes, nms_thresh, obj_thresh)\n",
    "\n",
    "  # Draw bounding boxes on the image using labels\n",
    "  image_detect = draw_boxes(image_pil, boxes, labels)\n",
    "\n",
    "  return image_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@36.993] global cap_gstreamer.cpp:1173 isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person 1.00 (553, 144) (1730, 1066)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 19:17:02.862 python[37957:1388621] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-23 19:17:02.862 python[37957:1388621] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person 1.00 (555, 141) (1725, 1070)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "person 1.00 (520, 158) (1752, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (506, 167) (1779, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (510, 173) (1772, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (526, 175) (1749, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (534, 177) (1739, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "toothbrush 0.56 (1145, 620) (1325, 714)\n",
      "person 1.00 (541, 190) (1723, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (537, 181) (1734, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (538, 173) (1741, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (541, 176) (1737, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (538, 176) (1740, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (524, 211) (1719, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (554, 258) (1648, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (529, 317) (1694, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (592, 385) (1617, 1075)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (624, 448) (1634, 1048)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (603, 495) (1609, 1051)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (582, 514) (1609, 1054)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (556, 533) (1621, 1058)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (574, 538) (1603, 1065)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (600, 540) (1594, 1066)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (594, 540) (1610, 1065)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (617, 530) (1600, 1053)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 0.72 (476, 853) (734, 1080)\n",
      "person 1.00 (656, 508) (1628, 1054)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (646, 479) (1624, 1063)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "person 1.00 (637, 412) (1638, 1072)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (594, 374) (1670, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (557, 353) (1671, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (562, 327) (1655, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (543, 297) (1636, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (525, 267) (1647, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "person 1.00 (523, 250) (1642, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (501, 240) (1652, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (495, 232) (1663, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (500, 233) (1663, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (497, 229) (1662, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (502, 226) (1656, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (507, 240) (1664, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (502, 238) (1678, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (514, 233) (1671, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "person 1.00 (529, 226) (1678, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (540, 226) (1677, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (532, 216) (1685, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (531, 210) (1686, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (530, 210) (1680, 1080)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "person 1.00 (522, 213) (1690, 1080)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def capture_frame(cap):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return None\n",
    "    # Convert frame from BGR (OpenCV) to RGB (PIL)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_pil = Image.fromarray(frame_rgb)\n",
    "    return image_pil\n",
    "\n",
    "def run_webcam_detection():\n",
    "    cap = cv2.VideoCapture(0)  # Open the default webcam\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        image_pil = capture_frame(cap)\n",
    "        if image_pil is None:\n",
    "            break\n",
    "\n",
    "        # Use your YOLO detection function\n",
    "        result_pil = detect_image(image_pil)\n",
    "        \n",
    "        # Convert the result (PIL image) back to OpenCV format for display\n",
    "        result_np = np.array(result_pil)\n",
    "        result_bgr = cv2.cvtColor(result_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        cv2.imshow(\"Webcam Object Detection\", result_bgr)\n",
    "\n",
    "        # Exit when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "run_webcam_detection()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
